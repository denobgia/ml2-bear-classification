{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhVwe3j4OCsY"
      },
      "source": [
        "# 1. Project Goal/Motivation\n",
        "\n",
        "## Goal:\n",
        "Develop an image classification model that can accurately identify different species of bears from images.\n",
        "\n",
        "## Motivation:\n",
        "- **Problem**: Different bear species exhibit different behaviors, and appropriate human responses to encounters with them vary. Accurate identification can help in promoting safety and effective wildlife management.\n",
        "- **Relevance**: This project can aid hikers, wildlife enthusiasts, and conservationists in identifying bear species quickly and accurately, contributing to safer interactions and better conservation strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZA7s1rXOG_r"
      },
      "source": [
        "# 2. Data\n",
        "\n",
        "## Data Collection:\n",
        "- The dataset for this project consists of images of various bear species sourced from a publicly available dataset.\n",
        "\n",
        "## Categories:\n",
        "- The dataset includes images of common bear species such as:\n",
        "  1. Polar bears\n",
        "  2. Grizzly bears\n",
        "  3. Black bears\n",
        "  4. Panda bears\n",
        "  5. Teddy bears\n",
        "\n",
        "## Data Preparation:\n",
        "- Image preprocessing steps were applied to ensure consistency and quality:\n",
        "  - Image resizing: Images were resized to a standardized dimension (e.g., 224x224 pixels).\n",
        "  - Data augmentation: Techniques like rotation, zoom, and flipping were applied to increase dataset variability and improve model robustness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZaDcMQ8pqIkg"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import os  # Provides functions to interact with the operating system\n",
        "import sys  # Provides access to some variables used or maintained by the interpreter\n",
        "import shutil  # Offers high-level operations on files and collections of files\n",
        "\n",
        "from tempfile import NamedTemporaryFile  # Creates temporary files and directories\n",
        "from urllib.request import urlopen  # Opens URLs\n",
        "from urllib.parse import unquote, urlparse  # Provides utilities to handle URL parsing and quoting\n",
        "from urllib.error import HTTPError  # Defines the exceptions raised by urllib\n",
        "\n",
        "from zipfile import ZipFile  # Provides tools to create, read, write, append, and list a ZIP file\n",
        "import tarfile  # Provides tools to read and write tar archive files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsnRZHlcMC70",
        "outputId": "5b51f4ef-ea9e-4a38-eb69-df2344f90855"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: '/kaggle'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input\u001b[39m\u001b[38;5;124m'\u001b[39m, ignore_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create the /kaggle/input directory with full permissions\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKAGGLE_INPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0o777\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create the /kaggle/working directory with full permissions\u001b[39;00m\n\u001b[1;32m     19\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(KAGGLE_WORKING_PATH, \u001b[38;5;241m0o777\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle'"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "\n",
        "CHUNK_SIZE = 40960  # Define the size of each chunk to be read from the URL\n",
        "\n",
        "# Mapping of dataset name to its encoded URL\n",
        "DATA_SOURCE_MAPPING = 'bear-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2293433%2F3855739%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240601%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240601T163604Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3d149c913c839d42dfc99190066b0d8dc0cc4495ee921249978a937b91ca68edde89dcc26addea3eec9cb14b621d51c7b3752465fed72bca66931c40fd6c18eea1b231b9ae99115da46ecc9e87804216aea188f333e62797a8487f367256275409ca24aafbf8a43f3e54bb5b8c26dc9f862378bf5ea390d07982ef89882d5639e2a6b74441c64877446562b964a7d22e043bbaa2a63fe0bab3ae4b6b9618b79f181592254144fa21e75e2a6d56a96d82c4a1814ce425394aec30006b7e8522f7f6c073b414f2c12be6f80ef432802558edf37a479f2baff4f8bbbc10f58ef02380f536f916c584edf96d43f26a28328fdc8ae3068c9a7b4e230449e1c6ad0516'\n",
        "\n",
        "KAGGLE_INPUT_PATH = '/kaggle/input'  # Define the input path for Kaggle\n",
        "KAGGLE_WORKING_PATH = '/kaggle/working'  # Define the working path for Kaggle\n",
        "KAGGLE_SYMLINK = 'kaggle'  # Define the symlink name for Kaggle\n",
        "\n",
        "# Unmount the /kaggle/input directory if it is mounted\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "# Remove the /kaggle/input directory, ignoring errors if it does not exist\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "# Create the /kaggle/input directory with full permissions\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "# Create the /kaggle/working directory with full permissions\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "# Create a symbolic link to the input directory, ignore if it already exists\n",
        "try:\n",
        "    os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "# Create a symbolic link to the working directory, ignore if it already exists\n",
        "try:\n",
        "    os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "\n",
        "# Loop through each data source mapping\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')  # Split into directory and encoded URL\n",
        "    download_url = unquote(download_url_encoded)  # Decode the URL\n",
        "    filename = urlparse(download_url).path  # Parse the filename from the URL\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)  # Define the destination path\n",
        "\n",
        "    try:\n",
        "        # Open the URL and create a temporary file\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']  # Get the total length of the file\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0  # Initialize the downloaded length\n",
        "            data = fileres.read(CHUNK_SIZE)  # Read the first chunk\n",
        "            while len(data) > 0:  # Continue reading until no more data\n",
        "                dl += len(data)  # Increment the downloaded length\n",
        "                tfile.write(data)  # Write the data to the temporary file\n",
        "                done = int(50 * dl / int(total_length))  # Calculate the download progress\n",
        "                # Display the download progress\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)  # Read the next chunk\n",
        "\n",
        "            # Check if the file is a ZIP archive and extract it\n",
        "            if filename.endswith('.zip'):\n",
        "                with ZipFile(tfile) as zfile:\n",
        "                    zfile.extractall(destination_path)\n",
        "            # Otherwise, treat it as a tar archive and extract it\n",
        "            else:\n",
        "                with tarfile.open(tfile.name) as tarfile:\n",
        "                    tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')  # Confirm download and extraction\n",
        "    except HTTPError as e:\n",
        "        # Handle HTTP errors (likely due to an expired URL)\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        # Handle OS errors\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')  # Indicate that the data import is complete\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-27T06:26:50.023615Z",
          "iopub.status.busy": "2022-06-27T06:26:50.023151Z",
          "iopub.status.idle": "2022-06-27T06:27:02.468877Z",
          "shell.execute_reply": "2022-06-27T06:27:02.467463Z",
          "shell.execute_reply.started": "2022-06-27T06:26:50.023519Z"
        },
        "id": "z-Xp2cCPqIkk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for model\n",
        "\n",
        "# Building deep learning models\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# For accessing pre-trained models\n",
        "import tensorflow_hub as hub\n",
        "# For separating train and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import PIL.Image as Image\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-27T06:27:02.47166Z",
          "iopub.status.busy": "2022-06-27T06:27:02.470961Z",
          "iopub.status.idle": "2022-06-27T06:27:02.482278Z",
          "shell.execute_reply": "2022-06-27T06:27:02.481123Z",
          "shell.execute_reply.started": "2022-06-27T06:27:02.471616Z"
        },
        "id": "t5_SHRuOqIkk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Accessing the images link\n",
        "data_dir = \"./input/bear-dataset/data\" # Datasets path\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "data_dir\n",
        "\n",
        "# Opening each folder in a variable\n",
        "black = list(data_dir.glob('black/*'))\n",
        "grizzly = list(data_dir.glob('grizzly/*'))\n",
        "panda = list(data_dir.glob('panda/*'))\n",
        "polar = list(data_dir.glob('polar/*'))\n",
        "teddy = list(data_dir.glob('teddy/*'))\n",
        "\n",
        "# Assigning dirs for images and their labels\n",
        "# Contains the images path\n",
        "df_images = {\n",
        "    'black' : black,\n",
        "    'grizzly' : grizzly,\n",
        "    'panda' : panda,\n",
        "    'polar' : polar,\n",
        "    'teddy': teddy\n",
        "}\n",
        "\n",
        "# Contains numerical labels for the categories\n",
        "df_labels = {\n",
        "    'black' : 0,\n",
        "    'grizzly' : 1,\n",
        "    'panda' : 2,\n",
        "    'polar' : 3,\n",
        "    'teddy': 4\n",
        "}\n",
        "\n",
        "# Reshape dimensions 224x224\n",
        "X, y = [], [] # X = images, y = labels\n",
        "for label, images in df_images.items():\n",
        "    for image in images:\n",
        "        img = cv2.imread(str(image))\n",
        "        resized_img = cv2.resize(img, (224, 224)) # Resizing the images to be able to pass on MobileNetv2 model\n",
        "        X.append(resized_img)\n",
        "        y.append(df_labels[label])\n",
        "\n",
        "# Standarizing\n",
        "X = np.array(X)\n",
        "X = X/255\n",
        "y = np.array(y)\n",
        "\n",
        "# Training and Test Dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q2AEYTFP1rE"
      },
      "source": [
        "# 3. Model\n",
        "\n",
        "### TensorFlow Hub - MobileNetV2\n",
        "In this section of the notebook, we are utilizing a pre-trained MobileNetV2 model from TensorFlow Hub as the base for our neural network. MobileNetV2 is a lightweight, efficient convolutional neural network commonly used for feature extraction in image classification tasks. By importing the model without its final classification layer, we can leverage its pre-trained weights to extract high-level features from our input images.\n",
        "\n",
        "We initialize the MobileNetV2 model as a non-trainable Keras layer to retain its pre-trained weights during our training process. This approach, known as transfer learning, allows us to build a more effective model with reduced computational cost and training time, especially beneficial when working with smaller datasets.\n",
        "\n",
        "We then construct a new sequential model by adding a dense layer on top of the MobileNetV2 base. This dense layer, configured to match the number of target classes in our specific task, will be trained to perform the final classification.\n",
        "\n",
        "By compiling and summarizing this model, we prepare it for subsequent training and evaluation, effectively customizing a state-of-the-art image classification model for our unique dataset.\n",
        "\n",
        "We compile the model using the Adam optimizer and `SparseCategoricalCrossentropy` loss function, with accuracy as the evaluation metric. The model is trained on the training dataset for 10 epochs to adjust its weights and minimize the loss. This process generates a history object that tracks the model's performance and learning progression over the epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-06-27T06:27:07.75122Z",
          "iopub.status.busy": "2022-06-27T06:27:07.750731Z",
          "iopub.status.idle": "2022-06-27T06:27:09.565397Z",
          "shell.execute_reply": "2022-06-27T06:27:09.56416Z",
          "shell.execute_reply.started": "2022-06-27T06:27:07.75119Z"
        },
        "id": "KvPe2HTlqIkn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "mobile_net = 'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4' # MobileNetv4 link\n",
        "mobile_net = hub.KerasLayer(\n",
        "        mobile_net, input_shape=(224,224, 3), trainable=False) # Removing the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-06-27T06:27:09.570795Z",
          "iopub.status.busy": "2022-06-27T06:27:09.569736Z",
          "iopub.status.idle": "2022-06-27T06:27:10.605034Z",
          "shell.execute_reply": "2022-06-27T06:27:10.603768Z",
          "shell.execute_reply.started": "2022-06-27T06:27:09.570741Z"
        },
        "id": "H4zkKpXBqIkn",
        "outputId": "e8c01d0c-e9f5-404f-cb05-d8bd7fe4ef70",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_label = 5 # number of labels\n",
        "\n",
        "# Wrap the mobile_net layer in a Lambda layer to make it compatible with Sequential\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Lambda(lambda x: mobile_net(x)),\n",
        "    keras.layers.Dense(num_label)\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-06-27T06:27:10.609779Z",
          "iopub.status.busy": "2022-06-27T06:27:10.609062Z",
          "iopub.status.idle": "2022-06-27T06:27:39.177899Z",
          "shell.execute_reply": "2022-06-27T06:27:39.176995Z",
          "shell.execute_reply.started": "2022-06-27T06:27:10.609715Z"
        },
        "id": "vMSfqkWPqIkn",
        "outputId": "1c9da4d2-1665-4fb1-f230-9040756a0f51",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 162ms/step - acc: 0.3662 - loss: 1.4493\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 158ms/step - acc: 0.8168 - loss: 0.6211\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - acc: 0.9543 - loss: 0.2832\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - acc: 0.9832 - loss: 0.1732\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - acc: 0.9980 - loss: 0.1328\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - acc: 0.9965 - loss: 0.0934\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - acc: 1.0000 - loss: 0.0799\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - acc: 1.0000 - loss: 0.0691\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - acc: 1.0000 - loss: 0.0607\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - acc: 1.0000 - loss: 0.0459\n"
          ]
        }
      ],
      "source": [
        "# Training the model\n",
        "model.compile(\n",
        "  optimizer=\"adam\",\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['acc'])\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHcR-HRbqIkn"
      },
      "source": [
        "# 4. Interpretation and Validation\n",
        "\n",
        "We evaluate the model's performance on the test dataset to obtain its accuracy and other metrics. Subsequently, we generate predictions for the test data and convert these predictions to class labels. Finally, we print a detailed classification report, including precision, recall, and F1-score for each class, using the true labels and predicted labels.\n",
        "\n",
        "These results indicate that the model achieved a relatively low loss of 0.1740 and a high accuracy of 91.94% during training. The evaluation on the test dataset confirms the model's performance, with a similar loss of 0.17397 and accuracy of 91.93%.\n",
        "\n",
        "The classification report provides more detailed insights into the model's performance across different classes. Notably, class 0 has perfect precision but lower recall, suggesting that the model correctly identifies instances of this class but may miss some. Class 1 also has high precision and recall, indicating good performance. However, class 2 shows lower precision and recall, suggesting potential challenges in correctly classifying instances of this class. Classes 3 and 4 have high precision and recall, indicating that the model performs well on these classes.\n",
        "\n",
        "Overall, the model achieves an accuracy of 92% on the test dataset, with a balanced performance across most classes. However, class 2 appears to be more challenging for the model, possibly due to imbalanced data or inherent difficulties in distinguishing instances of this class. Further analysis, such as investigating misclassified instances or exploring different model architectures, may help improve the model's performance, particularly for class 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-06-27T06:27:39.180392Z",
          "iopub.status.busy": "2022-06-27T06:27:39.179866Z",
          "iopub.status.idle": "2022-06-27T06:27:40.819403Z",
          "shell.execute_reply": "2022-06-27T06:27:40.818157Z",
          "shell.execute_reply.started": "2022-06-27T06:27:39.180333Z"
        },
        "id": "WwE5GqnbqIkn",
        "outputId": "8c486f62-917b-4211-cc3d-0007c7d73a6e",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - acc: 0.9677 - loss: 0.1135\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.135707825422287, 0.9516128897666931]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-06-27T06:27:40.822172Z",
          "iopub.status.busy": "2022-06-27T06:27:40.821025Z",
          "iopub.status.idle": "2022-06-27T06:27:42.399018Z",
          "shell.execute_reply": "2022-06-27T06:27:42.397862Z",
          "shell.execute_reply.started": "2022-06-27T06:27:40.82211Z"
        },
        "id": "eIXu3cPOqIkn",
        "outputId": "847b8622-9b50-44f1-fdbb-5f93e1142951",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       1.00      1.00      1.00        10\n",
            "           2       0.86      0.75      0.80         8\n",
            "           3       0.96      1.00      0.98        22\n",
            "           4       0.89      0.89      0.89         9\n",
            "\n",
            "    accuracy                           0.95        62\n",
            "   macro avg       0.94      0.93      0.93        62\n",
            "weighted avg       0.95      0.95      0.95        62\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(X_test, batch_size=64, verbose=1)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(classification_report(y_test, y_pred_bool))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
